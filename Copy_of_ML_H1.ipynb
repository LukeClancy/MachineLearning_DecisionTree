{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML.H1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukeClancy/MachineLearning_DecisionTree/blob/master/Copy_of_ML_H1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XgxULieqjM0n",
        "colab_type": "code",
        "outputId": "5692aca3-b15b-4be5-ae53-f78916735376",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1905
        }
      },
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "from google.colab import files\n",
        "import time\n",
        "\n",
        "# Calculate the Gini index for a subset of the dataset\n",
        "#def gini_index(groups, classes):\n",
        "#   # count all samples at split point\n",
        "#   num_instances = float(sum([len(group) for group in groups]))\n",
        "\n",
        "#   gini = 0.0 # sum weighted Gini index for each group\n",
        "#   for group in groups:\n",
        "#      size = float(len(group))\n",
        "#      if size == 0: # avoid divide by zero\n",
        "#         continue\n",
        "#      score = 0.0\n",
        "#      # score the group based on the score for each class\n",
        "#      for class_val in classes:\n",
        "#         p = [row[-1] for row in group].count(class_val) / size\n",
        "#         score += p * p\n",
        "      # weight the group score by its relative size\n",
        "#      gini += (1.0 - score) * (size / num_instances)\n",
        "#   return gini\n",
        "\n",
        "#credit where credit due: https://stackoverflow.com/questions/889900/accurate-timing-of-functions-in-python\n",
        "def timeWrapper(method):\n",
        "  def wrap(*args, **kw):\n",
        "    strt = int(time.time() * 1000)\n",
        "    result = method(*args, **kw)\n",
        "    endTime = int(time.time() * 1000)\n",
        "    print(\"time was: \" + str(endTime - strt))\n",
        "    return result\n",
        "  return wrap\n",
        "    \n",
        "\n",
        "def info_gain(groups, classes):\n",
        "  num_instances = float(sum([len(group) for group in groups]))\n",
        "  infGain = float(0)\n",
        "  for group in groups:\n",
        "    size = float(len(group))\n",
        "    if size == 0:\n",
        "      continue\n",
        "    #for row in group:\n",
        "    #  posies += row[-1]\n",
        "    #negies = float(size - posies)    \n",
        "    entropy = float(0)\n",
        "    for classy in classes: #in this case only two\n",
        "      count = [row[-1] for row in group].count(classy)\n",
        "      if count != 0:\n",
        "        entropy -= (count/size) * log(count/size, 2)\n",
        "    infGain += (size / num_instances) * (1 - entropy)\n",
        "  return infGain\n",
        "  \n",
        "# Create child splits for a node or make a leaf node\n",
        "def split(node, max_depth, depth):\n",
        "   print('in split len groups: ' + str(len(node['groups'])))\n",
        "   print('len grp 0: ' + str(len(node['groups'][0])) + \", grp 1: \" + str(len(node['groups'][1])))\n",
        "   left, right = node['groups']\n",
        "   del(node['groups'])\n",
        "   # check for a no split\n",
        "   if not left or not right:\n",
        "      print('in check for no split')\n",
        "      node['left'] = node['right'] = create_leaf(left + right)\n",
        "      return\n",
        "   # check for max depth\n",
        "   if depth >= max_depth:\n",
        "      print('in check for max depth')\n",
        "      node['left'], node['right'] = create_leaf(left), create_leaf(right)\n",
        "      return\n",
        "   print('selecting left')\n",
        "   node['left'] = select_attribute(left)\n",
        "   split(node['left'], max_depth, depth+1)\n",
        "   print('selecting right')\n",
        "   node['right'] = select_attribute(right)\n",
        "   split(node['right'], max_depth, depth+1)\n",
        "\n",
        "\n",
        "# split the dataset based on an attribute and attribute value\n",
        "def test_split(index, value, dataset):\n",
        "   left, right = list(), list()\n",
        "   for row in dataset:\n",
        "      if row[index] < value:\n",
        "         left.append(row)\n",
        "      else:\n",
        "         right.append(row)\n",
        "   return left, right\n",
        "\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "@timeWrapper\n",
        "def select_attribute(dataset):\n",
        "   print(\"len selecting: \" + str(len(dataset)))\n",
        "   class_values = list(set(row[-1] for row in dataset))\n",
        "   b_index, b_value, b_score, b_groups = 999, 999, 0, None\n",
        "   for index in range(len(dataset[0])-1):\n",
        "      for row in dataset:\n",
        "         groups = test_split(index, row[index], dataset)\n",
        "         infg = info_gain(groups, class_values)\n",
        "         if infg > b_score: #changed < to >\n",
        "            b_index, b_value, b_score, b_groups = index, row[index], infg, groups\n",
        "      #if index % 100 == 0:\n",
        "        #print(\"index \" + str(index) + \" completed\")\n",
        "   return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "# Create a leaf node class value\n",
        "def create_leaf(group):\n",
        "   outcomes = [row[-1] for row in group]\n",
        "   return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth):\n",
        "   print(\"len train: \" + str(len(train)))\n",
        "   root = select_attribute(train)\n",
        "   print(\"attr selected\")\n",
        "   split(root, max_depth, 1)\n",
        "   return root\n",
        "  \n",
        "  \n",
        "# Print a decision tree\n",
        "def print_tree(node, depth=0):\n",
        "   if depth == 0:\n",
        "      print 'Tree:'\n",
        "   if isinstance(node, dict):\n",
        "      print('%s[X%d < %.3f]' % (depth*' ', (float(node['index'])+1), float(node['value'])))\n",
        "      print_tree(node['left'], depth+1)\n",
        "      print_tree(node['right'], depth+1)\n",
        "   else:\n",
        "      print('%s[%s]' % ((depth*' ', node)))\n",
        "       \n",
        "   # Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "   if row[node['index']] < node['value']:\n",
        "      if isinstance(node['left'], dict):\n",
        "         return predict(node['left'], row)\n",
        "      else:\n",
        "         return node['left']\n",
        "   else:\n",
        "      if isinstance(node['right'], dict):\n",
        "         return predict(node['right'], row)\n",
        "      else:\n",
        "         return node['right']\n",
        "\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "   print('available files:')\n",
        "   !ls\n",
        "   _files = files.upload()\n",
        "   print('files entered ' + str(_files.keys()))\n",
        "   fileName = input(\"please enter filename chosen, it doesnt like dots so use \\\"s \")\n",
        "   myFile = open(fileName, \"r\")\n",
        "   line = myFile.readline()\n",
        "   print('past a, line:' + line)\n",
        "   dataset = []\n",
        "   while line and line != \"\":\n",
        "      dat = line.split(',')\n",
        "      for a in range(len(dat) - 1):\n",
        "            dat[a] = float(dat[a])\n",
        "      dataset.append(dat)\n",
        "      line = myFile.readline()\n",
        "   print('past b')\n",
        "   test_dat = dataset[0:len(dataset)/3]\n",
        "   train_dat = dataset[len(dataset)/3:]\n",
        "   tree = build_tree(train_dat, 4)\n",
        "   print_tree(tree)\n",
        "   correct = 0\n",
        "   failed = 0\n",
        "   for row in test_dat:\n",
        "      prediction = predict(tree, row)\n",
        "      if prediction == row[-1]:\n",
        "        correct+=1\n",
        "      else:\n",
        "        failed+=1\n",
        "      #print('Predicted=%d, Ground truth=%d' % (prediction, row[-1]))\n",
        "   print('correct - failed ratio: %d - %d' % (correct, failed))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "available files:\n",
            "har.csv  sample_data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b3912980-b2e7-4140-ad14-98dcc900130a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b3912980-b2e7-4140-ad14-98dcc900130a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "files entered []\n",
            "please enter filename chosen, it doesnt like dots so use \"s \"har.csv\"\n",
            "past a, line:1.4450396e-001,1.8926326e-001,6.2769317e-002,-9.0429967e-001,-1.8193654e-001,-4.4315051e-001,-9.0110019e-001,-1.1081268e-001,-4.0059935e-001,-9.3189578e-001,4.2098681e-002,-3.3652580e-001,7.1647623e-001,6.5581306e-001,8.0898559e-001,-5.3697289e-001,-9.9147358e-001,-7.9322292e-001,-8.1428623e-001,-8.9712813e-001,-1.5862471e-001,-4.0196407e-001,-4.8472227e-001,5.5583606e-001,1.7380770e-001,-4.1077609e-002,1.3076571e-002,7.3864994e-002,-3.1871724e-001,-2.7818848e-001,-1.0415121e-001,4.6032895e-001,-1.1399824e-001,-1.9921508e-001,-9.8337991e-002,5.2228156e-002,2.6252715e-001,-9.9084317e-001,-9.5033550e-001,9.8594537e-001,8.7409866e-001,4.2479149e-002,3.3309449e-001,-8.1418938e-001,-2.5461649e-001,-3.4244329e-001,-8.0702901e-001,-2.4906181e-001,-3.5495765e-001,8.6053314e-001,1.5158093e-001,4.0688316e-001,8.4957326e-001,-1.9754314e-001,1.1301998e-001,8.1578730e-002,6.7244755e-001,-9.5460358e-001,-7.6875065e-001,-7.8302306e-001,-2.4929859e-001,-4.4109235e-001,2.0080203e-001,1.2200461e-001,6.4780652e-001,-9.1393457e-001,9.1331398e-001,-9.1230496e-001,9.1109911e-001,-9.8417367e-001,9.8434769e-001,-9.8563708e-001,9.8737139e-001,-9.9857460e-001,9.9917535e-001,-9.9920661e-001,9.9522524e-001,-9.8517456e-001,-9.4523330e-001,9.8713302e-001,1.3805762e-001,-3.4777099e-001,-3.3527233e-001,-9.8122357e-001,-9.6614228e-001,-9.8349118e-001,-9.8447997e-001,-9.6488894e-001,-9.7987379e-001,-9.6486615e-001,-9.7899074e-001,-9.9776439e-001,9.8270097e-001,9.6303790e-001,9.7620391e-001,-9.7442362e-001,-9.9968427e-001,-9.9871193e-001,-9.9946696e-001,-9.8668169e-001,-9.6829749e-001,-9.7177895e-001,-5.0173008e-001,-8.7283765e-001,-9.5318134e-001,1.3984431e-001,2.2094286e-001,3.3887705e-001,8.9761526e-002,-1.9894960e-001,-5.0749274e-001,2.5579088e-001,-5.6015746e-002,-1.3679198e-001,-3.8947007e-001,-2.1709079e-001,-3.6811080e-001,6.3643890e-002,1.1902757e-001,3.9655649e-001,-4.6489666e-002,-2.8289794e-001,2.2948437e-001,-9.7335570e-001,-9.7220339e-001,-9.1719890e-001,-9.8014989e-001,-9.7861059e-001,-9.4088076e-001,-8.5690913e-001,-9.4748333e-001,-6.9418363e-001,8.2493137e-001,8.6506491e-001,7.5706536e-001,-8.6547659e-001,-9.9940450e-001,-9.9207096e-001,-9.8860902e-001,-9.9494456e-001,-9.8058068e-001,-9.7320862e-001,-7.0329136e-001,-9.5745183e-001,2.3084623e-001,-2.4606015e-001,8.2788632e-002,2.8377464e-001,-2.7487994e-001,-4.0866064e-001,2.8012775e-001,3.7178562e-003,-1.1427505e-001,-1.3361495e-001,-1.3374758e-001,3.2968591e-001,-5.5298436e-002,6.1039949e-001,-8.8554933e-001,-6.3115234e-001,-1.1571413e-001,-6.6663019e-002,1.6335990e-002,-9.8520162e-001,-9.8666522e-001,-9.8196657e-001,-9.8886052e-001,-9.9067775e-001,-9.8694566e-001,-9.8021352e-001,-9.8937313e-001,-9.4540482e-001,9.8225598e-001,9.8133991e-001,9.9359496e-001,-9.9032189e-001,-9.9983032e-001,-9.9987772e-001,-9.9971936e-001,-9.9265037e-001,-9.9528718e-001,-9.8950443e-001,-6.0168722e-001,-5.5881560e-001,-3.8181579e-001,-5.6538458e-002,5.1589501e-002,-3.5995299e-002,6.8223121e-001,-2.5841771e-001,1.2883566e-001,3.2380144e-001,-3.8748024e-001,1.5597544e-002,-3.4387209e-001,1.6838923e-001,-5.7665195e-002,8.2029223e-002,-5.4617551e-001,-2.0507984e-001,-5.5436920e-001,-1.8717165e-001,-2.0730115e-001,-4.6830669e-001,-8.7413908e-001,-5.5436920e-001,-8.3205491e-001,-2.6658477e-001,5.0515047e-001,-6.8011299e-001,3.1516201e-001,8.5490148e-002,1.7644387e-001,-5.5436920e-001,-1.8717165e-001,-2.0730115e-001,-4.6830669e-001,-8.7413908e-001,-5.5436920e-001,-8.3205491e-001,-2.6658477e-001,5.0515047e-001,-6.8011299e-001,3.1516201e-001,8.5490148e-002,1.7644387e-001,-9.7526798e-001,-9.7133698e-001,-9.7421791e-001,-9.7312196e-001,-9.6536371e-001,-9.7526798e-001,-9.9931329e-001,-9.7912720e-001,-5.7560394e-001,-1.6543358e-001,-3.0560045e-001,5.9776309e-001,-7.9118206e-002,-8.7388010e-001,-9.6976795e-001,-9.7075571e-001,-9.3837213e-001,-8.1114604e-001,-8.7388010e-001,-9.9268425e-001,-9.7584405e-001,-1.7026165e-002,-6.4471004e-001,6.8479915e-001,-5.2570146e-001,1.6056853e-001,-9.9023484e-001,-9.8011390e-001,-9.8343932e-001,-9.8229709e-001,-9.9107502e-001,-9.9023484e-001,-9.9984901e-001,-9.9480759e-001,-5.3051367e-001,4.9839782e-001,-2.6159451e-001,-4.9769820e-001,1.5542956e-001,-9.2128754e-001,-4.3277861e-001,-6.1401743e-001,-8.9802907e-001,-1.2526373e-001,-4.0381329e-001,-9.2114841e-001,-4.1505316e-001,-5.8077331e-001,-8.9152802e-001,-1.2515136e-001,-2.8630134e-001,-8.9347347e-001,-1.6371061e-001,-4.2664359e-001,-6.5843736e-001,-9.9502462e-001,-6.6049158e-001,-8.3828752e-001,-9.5787625e-001,-8.7966334e-001,-9.0507879e-001,-4.1123535e-001,-2.1696556e-001,-2.8803113e-001,-1.0000000e+000,-1.0000000e+000,-1.0000000e+000,-3.5489841e-001,-1.8146416e-001,-1.2436529e-001,5.0707305e-001,2.7792464e-001,4.5712351e-001,2.0766800e-001,3.9337846e-001,1.6612830e-001,-9.9353977e-001,-9.9944922e-001,-9.9816199e-001,-9.9910725e-001,-9.9837678e-001,-9.9693684e-001,-9.9698264e-001,-9.9522545e-001,-9.9468209e-001,-9.9816239e-001,-9.9785207e-001,-9.9639369e-001,-9.9493117e-001,-9.9833828e-001,-5.2606638e-001,-9.8127681e-001,-9.8709102e-001,-9.7267998e-001,-9.3341716e-001,-9.0842296e-001,-8.5127610e-001,-7.9074305e-001,-6.2723558e-001,-9.7963691e-001,-9.1622474e-001,-8.2089424e-001,-6.5706027e-001,-9.5374757e-001,-7.9731077e-001,-9.8472816e-001,-9.9258318e-001,-9.9167355e-001,-9.8463245e-001,-9.6102018e-001,-9.1690071e-001,-8.1874463e-001,-8.3307488e-001,-9.9228691e-001,-9.7744558e-001,-8.8677186e-001,-8.3751370e-001,-9.8763301e-001,-9.8209043e-001,-9.6914184e-001,-9.8156570e-001,-9.8187701e-001,-9.6489086e-001,-9.8385314e-001,-9.7858414e-001,-9.6713401e-001,-9.8451921e-001,-9.8658885e-001,-9.6061757e-001,-9.7399859e-001,-9.8704278e-001,-9.8547051e-001,-9.8540337e-001,-9.7662452e-001,-9.9969632e-001,-9.9910553e-001,-9.9962153e-001,-9.8370772e-001,-9.7200501e-001,-9.8727659e-001,-1.0000000e+000,-9.3912029e-001,-9.4221628e-001,-3.2000000e-001,-1.0000000e+000,-1.0000000e+000,2.5988055e-001,-5.9850344e-001,-2.7117090e-001,-2.7602662e-001,-7.7156238e-001,-1.4436705e-001,-5.6594391e-001,-3.2646948e-002,-2.9301605e-001,-9.9996941e-001,-9.9995736e-001,-9.9951817e-001,-9.9939548e-001,-9.9978776e-001,-9.9961827e-001,-9.9982158e-001,-9.9997105e-001,-9.9996039e-001,-9.9936222e-001,-9.9971411e-001,-9.9981911e-001,-9.9978756e-001,-9.9937362e-001,-9.9695575e-001,-9.9941960e-001,-9.9965310e-001,-9.9975804e-001,-9.9976596e-001,-9.9922045e-001,-9.9907522e-001,-9.9992612e-001,-9.9876362e-001,-9.9964188e-001,-9.9952400e-001,-9.9918657e-001,-9.9895000e-001,-9.9968652e-001,-9.9776757e-001,-9.9946482e-001,-9.9989876e-001,-9.9976861e-001,-9.9988779e-001,-9.9932806e-001,-9.9932106e-001,-9.9947829e-001,-9.9878275e-001,-9.9986463e-001,-9.9971909e-001,-9.9929669e-001,-9.9943218e-001,-9.9977526e-001,-9.6753600e-001,-9.6904380e-001,-8.9992251e-001,-9.7513158e-001,-9.7447678e-001,-9.3123209e-001,-9.6905851e-001,-9.7639779e-001,-9.1613914e-001,-9.7898612e-001,-9.7687277e-001,-9.4692630e-001,-9.9706082e-001,-9.4533236e-001,-8.6040184e-001,-9.5086318e-001,-9.9959379e-001,-9.9954168e-001,-9.9621298e-001,-9.7924822e-001,-9.7677070e-001,-9.3008272e-001,-5.1190757e-001,-4.4799535e-001,-2.0988466e-001,-1.0000000e+000,-1.0000000e+000,-1.0000000e+000,-6.5895342e-002,1.7024687e-001,1.3254114e-001,-2.9238355e-001,-6.9162730e-001,-2.5800110e-001,-6.3942222e-001,-2.9081625e-001,-6.6034939e-001,-9.9962785e-001,-9.9970902e-001,-9.9972927e-001,-9.9991811e-001,-9.9976973e-001,-9.9951537e-001,-9.9956302e-001,-9.9956038e-001,-9.9960748e-001,-9.9975091e-001,-9.9964440e-001,-9.9956186e-001,-9.9959972e-001,-9.9983916e-001,-9.9943892e-001,-9.9989461e-001,-9.9988794e-001,-9.9984351e-001,-9.9981366e-001,-9.9954245e-001,-9.9942137e-001,-9.9885304e-001,-9.9955447e-001,-9.9984806e-001,-9.9975647e-001,-9.9909637e-001,-9.9953318e-001,-9.9980456e-001,-9.9678570e-001,-9.9853887e-001,-9.9913236e-001,-9.9943699e-001,-9.9809353e-001,-9.9678941e-001,-9.9302941e-001,-9.9137662e-001,-9.9645449e-001,-9.9890275e-001,-9.9774114e-001,-9.9231105e-001,-9.9635642e-001,-9.9891271e-001,-4.4662992e-001,-1.9941645e-001,-3.5864225e-001,-2.2448761e-001,-1.9531749e-001,-4.4662992e-001,-6.6472232e-001,-8.6361006e-001,-1.0336335e-001,-1.0000000e+000,-3.3811215e-001,4.2832475e-001,1.9482511e-001,-9.7565857e-001,-9.6429842e-001,-9.6486513e-001,-9.6457770e-001,-9.6563918e-001,-9.7565857e-001,-9.9926642e-001,-9.8873555e-001,-9.4096500e-001,-1.0000000e+000,-3.1267252e-001,9.0920462e-002,-2.7887213e-001,-9.7506987e-001,-9.7112442e-001,-9.6532763e-001,-9.7797090e-001,-9.9379091e-001,-9.7506987e-001,-9.9943803e-001,-9.6291663e-001,-5.4048931e-001,-1.0000000e+000,-2.6760821e-001,-5.3302314e-001,-8.2853015e-001,-9.8331724e-001,-9.7721350e-001,-9.7963441e-001,-9.7699325e-001,-9.9576448e-001,-9.8331724e-001,-9.9974274e-001,-9.8371550e-001,-7.9588617e-001,-1.0000000e+000,-2.5760727e-001,1.5618530e-001,-2.4178130e-001,1.3525973e-002,4.3354353e-002,2.1485361e-002,4.6689153e-002,-6.6708465e-001,5.4216165e-002,-2.1887457e-001,1\n",
            "\n",
            "past b\n",
            "len train: 2333\n",
            "len selecting: 2333\n",
            "time was: 1479599\n",
            "attr selected\n",
            "in split len groups: 2\n",
            "len grp 0: 1217, grp 1: 1116\n",
            "selecting left\n",
            "len selecting: 1217\n",
            "time was: 294346\n",
            "in split len groups: 2\n",
            "len grp 0: 161, grp 1: 1056\n",
            "selecting left\n",
            "len selecting: 161\n",
            "time was: 4408\n",
            "in split len groups: 2\n",
            "len grp 0: 126, grp 1: 35\n",
            "selecting left\n",
            "len selecting: 126\n",
            "time was: 2764\n",
            "in split len groups: 2\n",
            "len grp 0: 86, grp 1: 40\n",
            "in check for max depth\n",
            "selecting right\n",
            "len selecting: 35\n",
            "time was: 276\n",
            "in split len groups: 2\n",
            "len grp 0: 0, grp 1: 35\n",
            "in check for no split\n",
            "selecting right\n",
            "len selecting: 1056\n",
            "time was: 219151\n",
            "in split len groups: 2\n",
            "len grp 0: 0, grp 1: 1056\n",
            "in check for no split\n",
            "selecting right\n",
            "len selecting: 1116\n",
            "time was: 243893\n",
            "in split len groups: 2\n",
            "len grp 0: 429, grp 1: 687\n",
            "selecting left\n",
            "len selecting: 429\n",
            "time was: 32486\n",
            "in split len groups: 2\n",
            "len grp 0: 92, grp 1: 337\n",
            "selecting left\n",
            "len selecting: 92\n",
            "time was: 1608\n",
            "in split len groups: 2\n",
            "len grp 0: 84, grp 1: 8\n",
            "in check for max depth\n",
            "selecting right\n",
            "len selecting: 337\n",
            "time was: 19274\n",
            "in split len groups: 2\n",
            "len grp 0: 0, grp 1: 337\n",
            "in check for no split\n",
            "selecting right\n",
            "len selecting: 687\n",
            "time was: 89399\n",
            "in split len groups: 2\n",
            "len grp 0: 0, grp 1: 687\n",
            "in check for no split\n",
            "Tree:\n",
            "[X84 < -0.721]\n",
            " [X1 < 0.270]\n",
            "  [X1 < 0.268]\n",
            "   [X1 < 0.262]\n",
            "    [1\n",
            "]\n",
            "    [1\n",
            "]\n",
            "   [X1 < 0.268]\n",
            "    [1\n",
            "]\n",
            "    [1\n",
            "]\n",
            "  [X1 < 0.270]\n",
            "   [1\n",
            "]\n",
            "   [1\n",
            "]\n",
            " [X1 < 0.263]\n",
            "  [X1 < 0.207]\n",
            "   [X1 < 0.205]\n",
            "    [0\n",
            "]\n",
            "    [0\n",
            "]\n",
            "   [X1 < 0.207]\n",
            "    [0\n",
            "]\n",
            "    [0\n",
            "]\n",
            "  [X1 < 0.263]\n",
            "   [0\n",
            "]\n",
            "   [0\n",
            "]\n",
            "correct - failed ratio: 1166 - 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-SItwDxTayFN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Luke Clancy\n",
        "Machine Learning\n",
        "Jan 18, 2019\n",
        "\n",
        "                                 Assignment #1\n",
        "\n",
        "1: I could not figure out how to paste an image in here, so this is the link \n",
        "\n",
        "https://drive.google.com/file/d/1IebFkhDMY5cO5K8F0QkztSvR-M4bCThppA/view?usp=sharing\n",
        "\n",
        "  1 - height/width ration of the 'o'\n",
        "  \n",
        "  2 - height/width ratio of the 'g'\n",
        "  \n",
        "  3 - tail of the 'u' apparent\n",
        "  \n",
        "  4 - tail of the 'g' passes the stem\n",
        "  \n",
        "  5 - angle at the end of 'g''s tail\n",
        "  \n",
        "  6 - overall smoothness vs rigidity of the lines\n",
        "  \n",
        "  7 - space and consistency of space between characters\n",
        "  \n",
        "  8 - bite size of the 'u' compared to the size of the character as a whole\n",
        "  \n",
        "  9 - string angling in a certain direction smoothly and consistently \n",
        "  \n",
        "  10 - consistency of the characters likenesses to eachother in general.\n",
        "  \n",
        "  ________________________________________________________________________\n",
        "\n",
        "2: (left for false right for true)\n",
        "\n",
        "  1 - \n",
        "\n",
        "                    A\n",
        "                  f/ \\t\n",
        "                  B  False\n",
        "                f/ \\t\n",
        "             True   False\n",
        "\n",
        "  2 - \n",
        "  \n",
        "                     C\n",
        "                   f/ \\t\n",
        "                   A   True\n",
        "                 f/  \\t\n",
        "               False   B\n",
        "                     f/ \\t\n",
        "                 False   True\n",
        "\n",
        " 3 - \n",
        " \n",
        "                      A\n",
        "                   f/    \\t\n",
        "                   B      B\n",
        "                f/ \\t    f/ \\t\n",
        "            False True  True False\n",
        "            \n",
        "________________________________________________________________________\n",
        "\n",
        "3:\n",
        "\n",
        "gain(F1) = 4/6\n",
        "\n",
        "gain(F2) = 0\n",
        "\n",
        "gain(F3) = 0\n",
        "\n",
        "We would choose F1 as it has the highest information gain. Notice that you can branch off the A option (what is stopping information gain from equalling one) with either F2 or F3 to complete the tree with an overall information gain of 1.\n",
        "\n",
        "notes, please disregard:\n",
        "\n",
        "   entropy = -(x1/xtotal * log2 (x1/xtotal) + x2/xtotal * log2(x1/xtotal) + ... )\n",
        "\n",
        "   information gain = 1 - xtotal /count (entropy(x)) + ytotal/count (entropy(y))...\n",
        "   \n",
        "_____________________________________________________________________________\n",
        "\n",
        "4:\n",
        "\n",
        "We can determine the extent of overfit by comparing the Accuracy peak to the accuracy(node count) we are comparing it to.\n",
        "\n",
        "Oa(x) = A(n) - A(x), where n is the nodecount where efficiency peaks (in this case 15)\n",
        "\n",
        "I woud choose nodecount of 15 a it is halfway between the two peaks, 10 and 20.\n",
        "\n",
        "_____________________________________________________________________________\n",
        "\n",
        "5:\n",
        "\n",
        "The performance is definitely better than a random guess, but it seems that it only needed one feature\n",
        "to reliably decide whether someone is standing or sitting. aka the tree only needed to be of depth 1. The feature that was used was the 84th feature.\n",
        "\n",
        "This was with a ratio of 1166 correct to 0 incorrect on the test data after being trained on the training data. \n",
        "\n",
        "I find this unusual although it may be a good lesson on the importance of features.\n",
        "\n",
        "______________________________________________________________________________\n"
      ]
    },
    {
      "metadata": {
        "id": "9fk7La4-jNze",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# \n",
        "\n",
        "**Homework Assignment #1**\n",
        "\n",
        "Assigned: January 8, 2019\n",
        "\n",
        "Due: January 17, 2019\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of four questions that require a short answer and one that requires you to generate some Python code. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for the your notebook as your homework submission.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        "(12 points) This question focuses on the topic of data collection and feature extraction. Consider a scenario in which we want to identify the person who hand-wrote a particular message. Collect data for this machine learning scenario by writing the word \"Cougs\" ten times. Also ask a friend or classmate to write this word ten times. Capture and upload an electronic image with these signatures. Analyzing these twenty images, write descriptions of at least ten specific features (types of strokes, lines, dots) that would best discriminate your handwriting from that of your friend.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(9 points) Generate decision trees that would represent the following boolean functions:\n",
        "\n",
        "\n",
        "\n",
        "*   not(A) and not(B)\n",
        "*   (A and B) or C\n",
        "*   A XOR B     (here \"XOR\" refers to exclusive or)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "(6 points) Assume that you are given the set of labeled training examples that appear below, where each attribute has possible values *a*, *b*, or *c*, and the target *Output* has values + or -. What is the information gain for each attribute in this dataset?\n",
        "\n",
        "\n",
        "F1 | F2 | F3 | Output\n",
        "--- | --- | --- | ---\n",
        "a | a | a | +\n",
        "c | b | c | +\n",
        "c | a | c | +\n",
        "b | a | a | -\n",
        "a | b | c | -\n",
        "b | b | c | -\n",
        "\n",
        "\n",
        "What feature would be chosen as the root of a decision tree?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "(12 points) Consider the training set accuracy and test set accuracy curves plotted in the graph below as a function of the decision tree size (number of nodes in the decision tree). From the accuracy value we can compute error as (1.0 - accuracy).\n",
        "\n",
        "![](https://drive.google.com/uc?id=1LVeFRdtI0cg9es6b4E12HO0AZSgn78fj)\n",
        "\n",
        "Can you suggest a way to determine the amount of overfit that exists in the learned decision tree model based on these curves? Explain / justify your answer.\n",
        "\n",
        "Based on the graph above what size decision tree would you choose to use and why?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#5.\n",
        "\n",
        "(80 points) In this problem you are asked to become familiar with python-based decision tree code and make modifications to the code. The code is based on a structure defined at machinelearningmastery.com. *Note that all of the code you write needs to be entirely your own, not copied from another existing program or using existing libraries that perform the specified functionality.*\n",
        "\n",
        "The first thing to note here is that the function used to determine the ideal attribute for splitting is gini index, rather than information gain as we discussed in class. Instead of using the entropy measure $-p^+ log_2 p^+ - p^- log_2 p^-$, we now use the gini measure $p^+(1-p^+) + p^-(1-p^-)$ . You can test out this function by adding these lines to the bottom of the code segment:\n",
        "\n",
        "print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))\n",
        "\n",
        "print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "z9Gq16k1izIC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate the Gini index for a subset of the dataset\n",
        "def gini_index(groups, classes):\n",
        "   # count all samples at split point\n",
        "   num_instances = float(sum([len(group) for group in groups]))\n",
        "\n",
        "   gini = 0.0 # sum weighted Gini index for each group\n",
        "   for group in groups:\n",
        "      size = float(len(group))\n",
        "      if size == 0: # avoid divide by zero\n",
        "         continue\n",
        "      score = 0.0\n",
        "      # score the group based on the score for each class\n",
        "      for class_val in classes:\n",
        "         p = [row[-1] for row in group].count(class_val) / size\n",
        "         score += p * p\n",
        "      # weight the group score by its relative size\n",
        "      gini += (1.0 - score) * (size / num_instances)\n",
        "   return gini"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SbV_IyrXjIPw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we define functions that test alternative attributes for splitting a node of the tree into two children (this code defines a binary decision tree). The test_split function divides the training data into two groups, one for each child. The get_split function determines the best attribute to split by calling test_split then evaluating the resulting data subsets using gini index.\n",
        "\n",
        "You may notice an important different between this decision tree and one we discussed in class. This tree has numeric features, rather than discrete features with symbolic names. This adds a new level of complexity to the idea of splitting the tree. Instead of creating a separate child for each discrete value of the feature, we split the entire numeric range into two partitions based on a threshold value (this is the row[index] value in the select_attribute function). Data points whose value for the selected attribute is < threshold are assigned to the left child of the split node, the remaining data points are assigned to the right child. Rather than test all possible threshold values (of which there are an infinite number) to determine which yields the best gini value, only the values that are found in the actual dataset are tested.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tWAIUJqWlODJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create child splits for a node or make a leaf node\n",
        "def split(node, max_depth, depth):\n",
        "   left, right = node['groups']\n",
        "   del(node['groups'])\n",
        "   # check for a no split\n",
        "   if not left or not right:\n",
        "      node['left'] = node['right'] = create_leaf(left + right)\n",
        "      return\n",
        "   # check for max depth\n",
        "   if depth >= max_depth:\n",
        "      node['left'], node['right'] = create_leaf(left), create_leaf(right)\n",
        "      return\n",
        "   node['left'] = select_attribute(left)\n",
        "   split(node['left'], max_depth, depth+1)\n",
        "   node['right'] = select_attribute(right)\n",
        "   split(node['right'], max_depth, depth+1)\n",
        "\n",
        "\n",
        "# split the dataset based on an attribute and attribute value\n",
        "def test_split(index, value, dataset):\n",
        "   left, right = list(), list()\n",
        "   for row in dataset:\n",
        "      if row[index] < value:\n",
        "         left.append(row)\n",
        "      else:\n",
        "         right.append(row)\n",
        "   return left, right\n",
        "\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def select_attribute(dataset):\n",
        "   class_values = list(set(row[-1] for row in dataset))\n",
        "   b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "   for index in range(len(dataset[0])-1):\n",
        "      for row in dataset:\n",
        "         groups = test_split(index, row[index], dataset)\n",
        "         gini = gini_index(groups, class_values)\n",
        "         if gini < b_score:\n",
        "            b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "   return {'index':b_index, 'value':b_value, 'groups':b_groups}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pc99NwMgcVLZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tBAmnqOdlyGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Because the dataset we will eventually use for this assignment is quite large, we add a max_depth parameter to the tree. As the tree is built, if the depth limit is reached the node is not split further.\n",
        "Similarly, if the subset of data at the current node is homogeneous (all the same class value), there is no need for a split. In either of these cases, the current node is considered a leaf node. The create_leaf function determines the class value that will be returned for that leaf node (based on the majority class label for training data that was assigned to the leaf node)."
      ]
    },
    {
      "metadata": {
        "id": "ixDly7OPmKC5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a leaf node class value\n",
        "def create_leaf(group):\n",
        "   outcomes = [row[-1] for row in group]\n",
        "   return max(set(outcomes), key=outcomes.count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YcR92JxUhK3r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As shown in the code below, this version of the program starts with a hard-coded dataset. This dataset contains two numeric attributes and one integer class value. The main function defines the dataset, calls a function to train the decision tree and print the resulting tree structure.  You can try running this and seeing what type of tree structure is learned from the example dataset. Try varying the value of max_depth and see how this affects the tree structure."
      ]
    },
    {
      "metadata": {
        "id": "A1VHmw6QgZXi",
        "colab_type": "code",
        "outputId": "d597a3df-417b-4e57-e52f-495b918445aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "cell_type": "code",
      "source": [
        "# Build a decision tree\n",
        "def build_tree(train, max_depth):\n",
        "   root = select_attribute(train)\n",
        "   split(root, max_depth, 1)\n",
        "   return root\n",
        "  \n",
        "  \n",
        "# Print a decision tree\n",
        "def print_tree(node, depth=0):\n",
        "   if depth == 0:\n",
        "      print 'Tree:'\n",
        "   if isinstance(node, dict):\n",
        "      print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
        "      print_tree(node['left'], depth+1)\n",
        "      print_tree(node['right'], depth+1)\n",
        "   else:\n",
        "      print('%s[%s]' % ((depth*' ', node)))\n",
        "      \n",
        "      \n",
        "if __name__ == \"__main__\":\n",
        "   dataset = [[2.771244718,1.784783929,0], [1.728571309,1.169761413,0],\n",
        "              [3.678319846,2.812813570,0], [3.961043357,2.619950320,0],\n",
        "              [2.999208922,2.209014212,0], [7.497545867,3.162953546,1],\n",
        "              [9.00220326, 3.339047188,1], [7.444542326,0.476683375,1],\n",
        "              [10.12493903,3.234550982,1], [6.642287351,3.319983761,1]]\n",
        "   tree = build_tree(dataset, 1)\n",
        "   print_tree(tree)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "value 2.771244718 groups ([[1.728571309, 1.169761413, 0]], [[2.771244718, 1.784783929, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [7.444542326, 0.476683375, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 1.728571309 groups ([], [[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [7.444542326, 0.476683375, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 3.678319846 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [2.999208922, 2.209014212, 0]], [[3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [7.444542326, 0.476683375, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 3.961043357 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [2.999208922, 2.209014212, 0]], [[3.961043357, 2.61995032, 0], [7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [7.444542326, 0.476683375, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 2.999208922 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0]], [[3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [7.444542326, 0.476683375, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 7.497545867 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.444542326, 0.476683375, 1], [6.642287351, 3.319983761, 1]], [[7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [10.12493903, 3.234550982, 1]])\n",
            "value 9.00220326 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.497545867, 3.162953546, 1], [7.444542326, 0.476683375, 1], [6.642287351, 3.319983761, 1]], [[9.00220326, 3.339047188, 1], [10.12493903, 3.234550982, 1]])\n",
            "value 7.444542326 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [6.642287351, 3.319983761, 1]], [[7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [7.444542326, 0.476683375, 1], [10.12493903, 3.234550982, 1]])\n",
            "value 10.12493903 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [7.444542326, 0.476683375, 1], [6.642287351, 3.319983761, 1]], [[10.12493903, 3.234550982, 1]])\n",
            "value 6.642287351 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0]], [[7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [7.444542326, 0.476683375, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 1.784783929 groups ([[1.728571309, 1.169761413, 0], [7.444542326, 0.476683375, 1]], [[2.771244718, 1.784783929, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 1.169761413 groups ([[7.444542326, 0.476683375, 1]], [[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 2.81281357 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.444542326, 0.476683375, 1]], [[3.678319846, 2.81281357, 0], [7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 2.61995032 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [2.999208922, 2.209014212, 0], [7.444542326, 0.476683375, 1]], [[3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 2.209014212 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [7.444542326, 0.476683375, 1]], [[3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 3.162953546 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.444542326, 0.476683375, 1]], [[7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 3.339047188 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.497545867, 3.162953546, 1], [7.444542326, 0.476683375, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]], [[9.00220326, 3.339047188, 1]])\n",
            "value 0.476683375 groups ([], [[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [7.444542326, 0.476683375, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 3.234550982 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.497545867, 3.162953546, 1], [7.444542326, 0.476683375, 1]], [[9.00220326, 3.339047188, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])\n",
            "value 3.319983761 groups ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0], [7.497545867, 3.162953546, 1], [7.444542326, 0.476683375, 1], [10.12493903, 3.234550982, 1]], [[9.00220326, 3.339047188, 1], [6.642287351, 3.319983761, 1]])\n",
            "Tree:\n",
            "[X1 < 6.642]\n",
            " [0]\n",
            " [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A-vztiJqnY7_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The procedure to learn a decision tree is now complete. To use the tree for prediction of a new data point, we define a function predict that feeds the data point (called \"row\") through the tree (rooted at \"node\"). The value of the leaf node that is reached is returned. You can try the prediction function out by adding these three lines to the end of the main function:\n",
        "\n",
        "   for row in dataset:\n",
        "      prediction = predict(tree, row)\n",
        "      print('Predicted=%d, Ground truth=%d' % (prediction, row[-1]))\n",
        "\n",
        "The predicted values will likely match the ground truth values. This is not surprising since data points are being tested that were also used to train the tree."
      ]
    },
    {
      "metadata": {
        "id": "J9yx6ndzoFU4",
        "colab_type": "code",
        "outputId": "9fd7a842-fb74-4398-b290-c3fe90cb1456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "cell_type": "code",
      "source": [
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "   if row[node['index']] < node['value']:\n",
        "      if isinstance(node['left'], dict):\n",
        "         return predict(node['left'], row)\n",
        "      else:\n",
        "         return node['left']\n",
        "   else:\n",
        "      if isinstance(node['right'], dict):\n",
        "         return predict(node['right'], row)\n",
        "      else:\n",
        "         return node['right']\n",
        "\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "   dataset = [[2.771244718,1.784783929,0], [1.728571309,1.169761413,0],\n",
        "              [3.678319846,2.812813570,0], [3.961043357,2.619950320,0],\n",
        "              [2.999208922,2.209014212,0], [7.497545867,3.162953546,1],\n",
        "              [9.00220326, 3.339047188,1], [7.444542326,0.476683375,1],\n",
        "              [10.12493903,3.234550982,1], [6.642287351,3.319983761,1]]\n",
        "   tree = build_tree(dataset, 3)\n",
        "   print_tree(tree)\n",
        "   for row in dataset:\n",
        "      prediction = predict(tree, row)\n",
        "      print('Predicted=%d, Ground truth=%d' % (prediction, row[-1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-be3a3e9d2e40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m               \u001b[0;34m[\u001b[0m\u001b[0;36m9.00220326\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.339047188\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m7.444542326\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.476683375\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m               [10.12493903,3.234550982,1], [6.642287351,3.319983761,1]]\n\u001b[0;32m---> 20\u001b[0;31m    \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m    \u001b[0mprint_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'build_tree' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "WAUoQfPboS64",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Your role:**\n",
        "\n",
        "In this homework assignment you need to make three changes to the code that has been provided.\n",
        "\n",
        "* First, replace the gini measure with the information gain measure we described in class.\n",
        "\n",
        "* Second, introduce functions that take the dataset and split into a subset that is used for training and a subset that is used for testing. The training set should represent 2/3 of the original data and testing will be the remaining 1/3 of the data. You can be as creative as you want in splitting the data into training and testing subsets. Now modify the main function to build the tree on the training data and test the predicted values on the testing data.  Instead of reporting each ground truth label, print the ratio of correctly-labeled testing data points (the predicted label matches the ground truth label) to the total number of testing data points.\n",
        "\n",
        "* Third, instead of using the hard coded dataset that is provided here, I would like you to train and test your tree on the human activity recognition dataset that we described in class. A description of the dataset is online at http://eecs.wsu.edu/~cook/ml/hw/har_readme.txt and a csv version of the data is available online at http://eecs.wsu.edu/~cook/ml/hw/har.csv. As we mentioned in class, each data point in this set represents features extracted from accelerometer and gyroscope data. The two class values are 0 (representing \"walking\") and 1 (representing \"sitting\"). Use the google library and upload_files function to upload the data and the numpy library to process the csv file, as shown in class.\n",
        "\n",
        "* Lastly, answer these questions. What observations can you make about the reported performance? Is it better than random guess?"
      ]
    }
  ]
}